### Motivation ###
Need an exterior penalty driver to run SU2 for FSI topology optimization with
multiple constraints and mechanisms for added reliability (retries or fallback)
and for data parallel execution (all adjoints at once).
Moreover, different settings may be needed for different runs.


### Assumptions ###
Functions are evaluated by writing variables and parameters to file, running
some commands and retrieving values from file.
These functions are much more expensive than the optimization.
The setup process is not meant to be generic at this stage, ad-hoc scripting
will be required, e.g. setting file names, run commands and so on, the
framework just aims to make this easier.


### Requirements ###
# General
- Exterior penalty method for any number of constraints and functions.
- Parallel or sequential evaluation of functions (and their gradients).
- Completely abstract, no reliance on SU2 parameters.
- A generic way to handle variables (even vector ones)
- Allow for manipulation of parameters, e.g. to implement ramping strategies.
- Keep results of each run.
- Files should be handled in the most generic of ways.

# Method specific
- Equality, inequality, and inner range constraints.
- Lazy evaluation of constraint gradients (only when needed).
- Ramping of penalty factors based on constraint violation tolerance.

# User experience
Create objects that represent the various components (functions, etc.).
Setup the driver with these objectives.
Pass it to the optimization.
(See example.py)


### Design ###
# World view
A DRIVER is composed of:
  - objectives
  - constraints
  - input variables (forming the design vector)
  - parameters (parameter vector)
It exposes methods to:
  - evaluate the penalized objective function
  - and its gradient
  - update penalties
  - setup all that is required

OBJECTIVES and CONTRAINTS decorate(?):
  FUNCTIONS
To add:
  - type (min/max, =, >, <, <.<)
  - scale
  - weight (for when objectives are combined)
  - bounds (of the constraints)
? These may not need to be classes, the function may contain its role.
  Since the driver handles the constraints it could make sense to set the role
  as the function is added to the driver.

FUNCTIONS are defined by:
  - output variable
  - and its gradient
  - value evaluation method
  - gradient evaluation method
It exposes methods to:
  - evaluate the function (taking the design vector and the parameters as args)
  - and its gradient

OUTPUT_VARIABLE:
  - value and gradient source files
  - parsing rules (maybe even a whole method)
(one output, one set of files, specified at construction)

INPUT_VARIABLE has:
  - initial value
  - current value
  - bounds
  - type (scalar or vector)
  - parsing rule to be written to file

PARAMETER is akin to input variable but:
  - does not need to be a number
  - is updated, e.g. incremented, rather than set by the optimizer

EVALUATION has:
  - config files (where inputs and parameters are set)
  - data files (mesh, initialization, etc.)
  - run instructions
Evaluation methods can be shared by functions and they should be lazy, i.e.
run only as needed.
Evaluations need to be chained to create pre/post processing steps like
deforming the grid. This may be achieved by adding a sequence of evaluations to
the function, without any explicit link other than the order.
What should constitute a run and what happens during it?
  - working directory (sub directory inside 
  - the input files above
  - output files? as they may be required for other operations of the pipeline,
    this may be done explicitly by the user 
The directory is created, the input files are moved into it (symbolic links are
more efficient but maybe not compatible with windows?), the Popen object is
created. (the evaluation parameters are written as the files are copied)
At this stage the config files can be updated by the function before the run.
(note: at some point it may make sense to reuse directories, update only config
       and make use of generated data without moving)

# File parsing methods
- Scalar input variables and parameters are parsed with the template and label
  strategy, i.e. the user provides a file with labels that will be replaced by
  values (the label is the parsing rule basically). In the future a label
  syntax can be developed to specify initial values and bounds in the template
  file to make the scripts less case-specific.

- Vector inputs and outputs require different strategies, these files have
  simpler formating, most of them can be treated as tables, one specifies row,
  column, separator(?) and header rows (to skip). For very special cases an
  entire user-defined method can be used.

# Parallel execution
This refers to the simultaneous evaluation of functions and their gradients,
not to the individual parallelization of those tasks.
An evaluation schedule needs to be created at the level of the evaluation
methods (not the function as those may share evaluations).
The optimization methods separate functions and gradient evaluation so that
dependency need not be established?
It is the derived driver that needs to create the schedule as some optimization
methods do not call for evaluation of all functions in one iteration.

# Ownership of variables
Should input variables and parameters be owned by functions of by the drivers?
The gradients need to be associated with input variables.
It would be convinient if we could pass the entire vector of parameters and
variables to the evaluation of a function. But at the same time it would be
nice to allow the same template file to be used for multiple functions, that
would require different parameters to have the same label, therefore each
function would need its own parameters.
Associating variables to functions is more cumbersome, it would require
duplication in the setup script and duplicates would have to be removed by the
driver.

Output variable may be dropped, the gradient parser needs to be associated
with input variables as they are added to a function or evaluation method,
which would leave the output variable class empty. Also the output file should
be set in the evaluation method so it can be managed by it.

# Data/control flow
What happens when the optimizer requests the driver to evaluate the objective
function.

Quick recap:
  - The driver holds functions;
  - The functions have evaluations (val & grad) and variables;
  - The evaluations have parameters and input files;

First let's make the sequential version work:
  - Each iteration takes place inside the working directory of the driver;
  - There each evaluation creates its own directory;
  - At the beginning of a new iteration the work dir may be saved;
  - The driver will evaluate the functions one by one, the functions direct the
    call to the evaluation method which will be lazy;



